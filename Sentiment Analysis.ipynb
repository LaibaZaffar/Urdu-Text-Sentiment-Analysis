{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbade34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.mendeley.com/datasets/rz3xg97rm5/1 (dataset source)\n",
    "#https://www.kaggle.com/datasets/rtatman/urdu-stopwords-list?select=stopwords-ur.txt(stopwords source)\n",
    "#https://sourceforge.net/projects/resource-for-urdu-stemmer/files/Urdu%20Affix%20lists.pdf/download (stop words,suffix,prefifix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600cac61",
   "metadata": {},
   "source": [
    "# Import Libraries/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97b3270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9466fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('C:/Users/admin/OneDrive/Desktop/SEMESTER7/NLP_ass#1/Urdu Tweets Dataset.csv',encoding='utf-8-sig')\n",
    "\n",
    "df = df.drop(['Category','Id','Emoticon'], axis=1)  #not needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66dc2b",
   "metadata": {},
   "source": [
    "# 1) STOP WORDS REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5bdc568",
   "metadata": {},
   "outputs": [],
   "source": [
    "urdu_stopwords = [\n",
    "    \"آئی\", \"آئے\", \"آج\", \"آخر\", \"آخرکبر\", \"آدهی\", \"آًب\", \"آٹھ\", \"آیب\", \"اة\", \n",
    "    \"اخبزت\", \"اختتبم\", \"ادھر\", \"ارد\", \"اردگرد\", \"ارکبى\", \"اش\", \"اضتعوبل\", \n",
    "    \"اضتعوبلات\", \"اضطرذ\", \"اضکب\", \"اضکی\", \"اضکے\", \"اطراف\", \"اغیب\", \"افراد\", \n",
    "    \"الگ\", \"اور\", \"اوًچب\", \"اوًچبئی\", \"اوًچی\", \"اوًچے\", \"اى\", \"اً\", \"اًذر\", \n",
    "    \"اًہیں\", \"اٹھبًب\", \"اپٌب\", \"اپٌے\", \"اچھب\", \"اچھی\", \"اچھے\", \"اکثر\", \"اکٹھب\", \n",
    "    \"اکٹھی\", \"اکٹھے\", \"اکیلا\", \"اکیلی\", \"اکیلے\", \"اگرچہ\", \"اہن\", \"ایطے\", \n",
    "    \"ایک\", \"ب\", \"ت\", \"تبزٍ\", \"تت\", \"تر\", \"ترتیت\", \"تریي\", \"تعذاد\", \"تن\", \"تو\", \n",
    "    \"توبم\", \"توہی\", \"توہیں\", \"تٌہب\", \"تک\", \"تھب\", \"تھوڑا\", \"تھوڑی\", \"تھوڑے\", \n",
    "    \"تھی\", \"تھے\", \"تیي\", \"ثب\", \"ثبئیں\", \"ثبترتیت\", \"ثبری\", \"ثبرے\", \"ثبعث\", \n",
    "    \"ثبلا\", \"ثبلترتیت\", \"ثبہر\", \"ثدبئے\", \"ثرآں\", \"ثراں\", \"ثرش\", \"ثعذ\", \"ثغیر\", \n",
    "    \"ثلٌذ\", \"ثلٌذوثبلا\", \"ثلکہ\", \"ثي\", \"ثٌب\", \"ثٌبرہب\", \"ثٌبرہی\", \"ثٌبرہے\", \n",
    "    \"ثٌبًب\", \"ثٌذ\", \"ثٌذکرو\", \"ثٌذکرًب\", \"ثٌذی\", \"ثڑا\", \"ثڑوں\", \"ثڑی\", \"ثڑے\", \n",
    "    \"ثھر\", \"ثھرا\", \"ثھراہوا\", \"ثھرپور\", \"ثھی\", \"ثہت\", \"ثہتر\", \"ثہتری\", \n",
    "    \"ثہتریي\", \"ثیچ\", \"ج\", \"خب\", \"خبرہب\", \"خبرہی\", \"خبرہے\", \"خبهوظ\", \"خبًب\", \n",
    "    \"خبًتب\", \"خبًتی\", \"خبًتے\", \"خبًٌب\", \"خت\", \"ختن\", \"خجکہ\", \"خص\", \"خططرذ\", \n",
    "    \"خلذی\", \"خو\", \"خواى\", \"خوًہی\", \"خوکہ\", \"خٌبة\", \"خگہ\", \"خگہوں\", \"خگہیں\", \n",
    "    \"خیطب\", \"خیطبکہ\", \"در\", \"درخبت\", \"درخہ\", \"درخے\", \"درزقیقت\", \"درضت\", \n",
    "    \"دش\", \"دفعہ\", \"دلچطپ\", \"دلچطپی\", \"دلچطپیبں\", \"دو\", \"دور\", \"دوراى\", \n",
    "    \"دوضرا\", \"دوضروں\", \"دوضری\", \"دوضرے\", \"دوًوں\", \"دکھبئیں\", \"دکھبتب\", \n",
    "    \"دکھبتی\", \"دکھبتے\", \"دکھبو\", \"دکھبًب\", \"دکھبیب\", \"دی\", \"دیب\", \"دیتب\", \n",
    "    \"دیتی\", \"دیتے\", \"دیر\", \"دیٌب\", \"دیکھو\", \"دیکھٌب\", \"دیکھی\", \"دیکھیں\", \n",
    "    \"دے\", \"ر\", \"راضتوں\", \"راضتہ\", \"راضتے\", \"رریعہ\", \"رریعے\", \"رکي\", \"رکھ\", \n",
    "    \"رکھب\", \"رکھتب\", \"رکھتبہوں\", \"رکھتی\", \"رکھتے\", \"رکھی\", \"رکھے\", \"رہب\", \n",
    "    \"رہی\", \"رہے\", \"ز\", \"زبصل\", \"زبضر\", \"زبل\", \"زبلات\", \"زبلیہ\", \"زصوں\", \n",
    "    \"زصہ\", \"زصے\", \"زقبئق\", \"زقیتیں\", \"زقیقت\", \"زکن\", \"زکویہ\", \"زیبدٍ\", \n",
    "    \"صبف\", \"صسیر\", \"صفر\", \"صورت\", \"صورتسبل\", \"صورتوں\", \"صورتیں\", \"ض\", \"ضبت\", \n",
    "    \"ضبتھ\", \"ضبدٍ\", \"ضبرا\", \"ضبرے\", \"ضبل\", \"ضبلوں\", \"ضت\", \"ضرور\", \"ضرورت\", \n",
    "    \"ضروری\", \"ضلطلہ\", \"ضوچ\", \"ضوچب\", \"ضوچتب\", \"ضوچتی\", \"ضوچتے\", \"ضوچو\", \n",
    "    \"ضوچٌب\", \"ضوچی\", \"ضوچیں\", \"ضکب\", \"ضکتب\", \"ضکتی\", \"ضکتے\", \"ضکٌب\", \n",
    "    \"ضکی\", \"ضکے\", \"ضیذھب\", \"ضیذھی\", \"ضیذھے\", \"ضیکٌڈ\", \"ضے\", \"طرف\", \"طریق\", \n",
    "    \"طریقوں\", \"طریقہ\", \"طریقے\", \"طور\", \"طورپر\", \"ظبہر\", \"ع\", \"عذد\", \"عظین\", \n",
    "    \"علاقوں\", \"علاقہ\", \"علاقے\", \"علاوٍ\", \"عووهی\", \"غبیذ\", \"غخص\", \"غذ\", \n",
    "    \"غروع\", \"غروعبت\", \"غے\", \"فرد\", \"فی\", \"ق\", \"قجل\", \"قجیلہ\", \"قطن\", \"لئے\", \n",
    "    \"لا\", \"لازهی\", \"لو\", \"لوجب\", \"لوجی\", \"لوجے\", \"لوسبت\", \"لوسہ\", \"لوگ\", \n",
    "    \"لوگوں\", \"لڑکپي\", \"لگتب\", \"لگتی\", \"لگتے\", \"لگٌب\", \"لگی\", \"لگیں\", \n",
    "    \"لگے\", \"لی\", \"لیب\", \"لیٌب\", \"لیں\", \"لے\", \"هتعلق\", \"هختلف\", \n",
    "    \"هسترم\", \"هسترهہ\", \"هسطوش\", \"هسیذ\", \"هطئلہ\", \"هطئلے\", \"هطبئل\", \n",
    "    \"هطتعول\", \"هطلق\", \"هعلوم\", \"هػتول\", \"هلا\", \"هوکي\", \"هوکٌبت\", \"هوکٌہ\", \n",
    "    \"هٌبضت\", \"هڑا\", \"هڑًب\", \"هڑے\", \"هکول\", \"هگر\", \"هہرثبى\", \"هیرا\", \"هیری\", \n",
    "    \"هیرے\", \"هیں\", \"و\", \"وار\", \"والے\", \"وٍ\", \"ًئی\", \"ًئے\", \"ًب\", \"ًبپطٌذ\", \n",
    "    \"ًبگسیر\", \"ًطجت\", \"ًقطہ\", \"ًو\", \"ًوخواى\", \"ًکبلٌب\", \"ًکتہ\", \"ًہ\", \n",
    "    \"ًہیں\", \"ًیب\", \"ًے\", \"ٓ آش\", \"ٹھیک\", \"پبئے\", \"پبش\", \"پبًب\", \"پبًچ\", \"پر\", \n",
    "    \"پراًب\", \"پطٌذ\", \"پل\", \"پورا\", \"پوچھب\", \"پوچھتب\", \"پوچھتی\", \"پوچھتے\", \n",
    "    \"پوچھو\", \"پوچھوں\", \"پوچھٌب\", \"پوچھیں\", \"پچھلا\", \"پھر\", \"پہلا\", \"پہلی\", \n",
    "    \"پہلےضی\", \"پہلےضے\", \"پہلےضےہی\", \"پیع\", \"چبر\", \"چبہب\", \"چبہٌب\", \n",
    "    \"چبہے\", \"چلا\", \"چلو\", \"چلیں\", \"چلے\", \"چکب\", \"چکی\", \"چکیں\", \"چکے\", \n",
    "    \"چھوٹب\", \"چھوٹوں\", \"چھوٹی\", \"چھوٹے\", \"چھہ\", \"چیسیں\", \"ڈھوًڈا\", \n",
    "    \"ڈھوًڈلیب\", \"ڈھوًڈو\", \"ڈھوًڈًب\", \"ڈھوًڈی\", \"ڈھوًڈیں\", \"ک\", \"کئی\", \"کئے\", \n",
    "    \"کب\", \"کبفی\", \"کبم\", \"کت\", \"کجھی\", \"کرا\", \"کرتب\", \"کرتبہوں\", \"کرتی\", \n",
    "    \"کرتے\", \"کرتےہو\", \"کررہب\", \"کررہی\", \"کررہے\", \"کرو\", \"کرًب\", \"کریں\", \n",
    "    \"کرے\", \"کطی\", \"کل\", \"کن\", \"کوئی\", \"کوتر\", \"کورا\", \"کوروں\", \"کورٍ\", \n",
    "    \"کورے\", \"کوطي\", \"کوى\", \"کوًطب\", \"کوًطی\", \"کوًطے\", \"کھولا\", \"کھولو\", \n",
    "    \"کھولٌب\", \"کھولی\", \"کھولیں\", \"کھولے\", \"کہ\", \"کہب\", \"کہتب\", \"کہتی\", \n",
    "    \"کہتے\", \"کہو\", \"کہوں\", \"کہٌب\", \"کہی\", \"کہیں\", \"کہے\", \"کی\", \"کیب\", \n",
    "    \"کیطب\", \"کیطرف\", \"کیطے\", \"کیلئے\", \"کیوًکہ\", \"کیوں\", \"کیے\", \"کے\", \n",
    "    \"کےثعذ\", \"کےرریعے\", \"گئی\", \"گئے\", \"گب\", \"گرد\", \"گروٍ\", \"گروپ\", \n",
    "    \"گروہوں\", \"گٌتی\", \"گی\", \"گیب\", \"گے\", \"ہر\", \"ہن\", \"ہو\", \"ہوئی\", \"ہوئے\", \n",
    "    \"ہوا\", \"ہوبرا\", \"ہوبری\", \"ہوبرے\", \"ہوتب\", \"ہوتی\", \"ہوتے\", \"ہورہب\", \n",
    "    \"ہورہی\", \"ہورہے\", \"ہوضکتب\", \"ہوضکتی\", \"ہوضکتے\", \"ہوًب\", \"ہوًی\", \n",
    "    \"ہوًے\", \"ہوچکب\", \"ہوچکی\", \"ہوچکے\", \"ہوگئی\", \"ہوگئے\", \"ہوگیب\", \"ہوں\", \n",
    "    \"ہی\", \"ہیں\", \"ہے\", \"ی\", \"یقیٌی\", \"یہ\", \"یہبں\",\n",
    "]\n",
    "urdu_stopwords1 = [\n",
    "     'آئی', 'آئے', 'آج', 'آدمی', 'آپ', 'اپنا', 'اپنی', 'اپنے', 'اکثر', 'اگر', 'اور', 'ایسا', 'ایسی',\n",
    "     'ایسے', 'ایک', 'بار', 'بعض', 'بعد', 'بتایا', 'بھی', 'بنا', 'بنانا', 'بنایا', 'بنے', 'برابر', 'بڑا',\n",
    "     'بڑی', 'بڑے', 'بہت', 'بے', 'تجھے', 'تھا', 'تھوڑا', 'تھوڑی', 'تھوڑے', 'تھی', 'تھے', 'جب', 'جس', 'جن',\n",
    "     'جنا', 'جنہیں', 'جو', 'حالانکہ', 'خبردار', 'خود', 'درمیان', 'دیر', 'دی', 'دیا', 'دیے', 'دو', 'دن',\n",
    "     'دوسرا', 'دوسری', 'دوسرے', 'دینا', 'رہا', 'رہے', 'رہی', 'ساتھ', 'سکتا', 'سکتی', 'سے', 'سو', 'ضرور',\n",
    "     'ظاہر', 'عدم', 'کر', 'کرتا', 'کرتی', 'کرتے', 'کیا', 'کئے', 'کرنا', 'کرنے', 'کو', 'کون', 'کیسا',\n",
    "     'کیسی', 'کیسے', 'کیوں', 'لئے', 'لگا', 'لیکن', 'مارا', 'میں', 'مگر', 'مجھے', 'مزید', 'مل', 'ملی', 'مناسب',\n",
    "     'نے', 'والا', 'والی', 'والے', 'وقت', 'وغیرہ', 'وہ', 'وہاں', 'وہی', 'ویسا', 'ویسے',\n",
    "     'ہر', 'ہم', 'ہمیشہ', 'ہمارا', 'ہماری', 'ہمارے', 'ہی', 'یہ', 'یہاں', 'یہی', 'اب', 'ادھر', 'ارد', 'اصل', \n",
    "     'اگرچہ', 'امید', 'انہیں', 'بجائے', 'بدستور', 'بلا', 'تم', 'تمہیں', 'تمہارا', 'تمہاری', 'تمہارے', 'تک', \n",
    "     'تھک', 'تو', 'توں', 'تین', 'جائے', 'جبکہ', 'جاتا', 'جاتی', 'جاتے', 'جا', 'جیسا', 'جیسے', 'خاطر', 'خلاف', \n",
    "     'خیال', 'در', 'دوران', 'ذرا', 'رکھ', 'رکھتا', 'رکھتی', 'رکھتے', 'رکھنا', 'رکھنے', 'صرف', 'عبارت', 'عین', \n",
    "     'غالب', 'غیر', 'فقط', 'قابل', 'قریب', 'کرکے', 'کہیں', 'کہ', 'لگ', 'لگتا', 'لگتی', 'لگتے', 'ملتا', \n",
    "     'ملتی', 'ملتے', 'نہایت', 'نیز', 'ہو', 'ہوتا', 'ہوتی', 'ہوتے', 'ہوا', 'ہوئی', 'ہوئے', 'ہونگے', 'ہے', 'ہیں', \n",
    "     'بالکل', 'تحت', 'تمام', 'جلدی', 'چونکہ', 'حاصل', 'درست', 'زیادہ', 'پہ', 'پہنچ', 'کافی', 'کام', 'کم', \n",
    "     'کب', 'کبھی', 'نزدیک', 'پھر', 'جہاں', 'کس', 'کن', 'کچھ', 'کسے', 'کے', 'گو', 'ہزار', 'ہرگز', 'یعنی', \n",
    "     'آئیگا', 'اک', 'ان', 'انہاں', 'اندر', 'ایس', 'باہر', 'جد', 'جدوں', 'جہڑا', 'جہڑے', 'سارے', 'ساڈا', 'سی', \n",
    "     'کدی', 'کردے', 'کردی', 'کرنا', 'کرن', 'کرو', 'کی', 'کیویں', 'نال', 'ہن', 'ہو', 'ہون', 'ہوں', 'بندہ', 'پر', 'جائے'\n",
    "     'کرے','وه','گیا ','بتا','ہے','ہا',' نے','اسے','ہے','ہاہاہاہاہاہاہاہا','ہاہاہاہاہاہا','اللہ',\n",
    " ]\n",
    "urdu_stopwords2= [\n",
    "    \"ک\", \"ی\", \"ل\", \"ی\", \"ے\", \"ئ\", \"ترا\", \"کہتے\", \"رہا\", \"تھوڑا\", \"جائےگی\", \"یدمز\", \"ہونا\", \n",
    "    \"یتھوڑ\", \"ہمیں\", \"کہتی\", \"رہاہے\", \"کسی\", \"کہتےہیں\", \"رہب\", \"توہی\", \"کرسکے\", \"یہیں\", \"ؤکھلا\", \n",
    "    \"رہتے\", \"توہیں\", \"کرنے\", \"ذرا\", \"یتر\", \"اتیر\", \"هالے\", \"آتے\", \"ہمی\", \"کہو\", \"رہتےہیں\", \"نہییو\", \n",
    "    \"آتی\", \"ےتیر\", \"بعض\", \"کھولو\", \"رہنے\", \"یتیر\", \"بعدازاں\", \"آتےہیں\", \"ترے\", \"کھولے\", \"رہے\", \n",
    "    \"هغیر\", \"آج\", \"یث\", \"بعضے\", \"کہوں\", \"رہی\", \"ذکره\", \"تو\", \"بڑھ\", \"یعہزر\", \"ث\", \"اسُ\", \"کہے\", \n",
    "    \"ہواکہ\", \"اچھب\", \"یعےزر\", \"ث\", \"فلاں\", \"کہی\", \"اسے\", \"کہیں\", \"زکن\", \"جاتا\", \"ہوئی\", \"اچھے\", \n",
    "    \"الہٰذ\", \"آخر\", \"دهیاز\", \"جارہاہے\", \"هںتیر\", \"کوتر\", \"اسے\", \"کورا\", \"سب\", \"جاؤ\", \"جلد\", \"ادھر\", \n",
    "    \"چند\", \"کورهں\", \"سکھائے\", \"جائے\", \"جلدہی\", \"اس\", \"ہمارا\", \"کوئی\", \"سکوں\", \"جائیں\", \"کرهں\", \n",
    "    \"اکٹھے\", \"یاکرد\", \"اکٹھی\", \"تھاکیا\", \"سیکھو\", \"جب\", \"انہیں\", \"ل\", \"چاہتے\", \"بھر\", \"اکثر\", \n",
    "    \"چاہتےہیں\", \"گذشتہ\", \"آگئے\", \"یعےکےزر\", \"انُہیں\", \"چاہیے\", \"سابقہ\", \"آم\", \"مختلف\", \"ی\", \n",
    "    \"یسیا\", \"یافرما\", \"چلتےبنے\", \"سکتا\", \"کرکے\", \"گرد\", \"هن\", \"گئے\", \"کاہے\", \"چلو\", \"جبکہ\", \n",
    "    \"آئے\", \"م\", \"چلے\", \"بارے\", \"آئی\", \"ہوچکے\", \"لگیں\", \"کرالو\", \"خوکہ\", \"گئی\", \"پ\", \"ہوچکی\", \n",
    "    \"لو\", \"کرتے\", \"دهر\", \"خو\", \"پڑتا\", \"ہورہے\", \"لوجب\", \"کرتی\", \"دهر\", \"خوامخواہ\", \"پڑتاہے\", \n",
    "    \"یو\", \"پڑتے\", \"ہورہی\", \"لوجے\", \"کرتےہو\", \"یاد\", \"آککر\", \"پھر\", \"هوگئے\", \"لوسبت\", \"کرتے\", \n",
    "    \"یتےد\", \"کہ\", \"پھرپوچھا\", \"هوگئی\", \"لوسہ\", \"کرتی\", \"پہلے\", \"سے\", \"یتید\", \"هونے\", \"لے\", \n",
    "    \"کرنا\", \"یرد\", \"ابھی\", \"پہنچ\", \"هوئے\", \"لی\", \"کرناپڑتیں\", \"یکهو\", \"پی\", \"پہنچوں\", \"لیتے\", \n",
    "    \"کرنی\", \"پہنچی\", \"یکهید\", \"پڑتی\", \"هورہی\", \"کرتاہے\", \"ہے\", \"پڑتے\", \"کرتےہیں\", \"را\", \"کئی\", \n",
    "    \"فرد\", \"چکیں\", \"سکتے\", \"کرکے\", \"موجود\", \"چکےہے\", \"مختلف\", \"پہنچ\", \"یرد\", \"کل\", \"کرتاہے\", \n",
    "    \"کبھی\", \"ہوچکے\", \"لگیں\", \"پڑتا\", \"ہورہے\", \"کرتےہیں\", \"پہنچ\", \"کبھی\", \"کرتاہے\", \"هالے\",\"ہیں\",\"ہے \",\n",
    "]\n",
    "\n",
    "combined_stopwords = set(urdu_stopwords + urdu_stopwords1 + urdu_stopwords2)# Combine and deduplicate stop words as set doesn't have duplicates\n",
    "combined_stopwords_list = list(combined_stopwords)\n",
    "\n",
    "# Now you can use combined_stopwords for filtering during preprocessing\n",
    "\n",
    "def remove_stopwords(text, stopwords, sentiment_words):\n",
    "    if isinstance(text, str):  \n",
    "        words = text.split()  \n",
    "        filtered_words = []\n",
    "        for word in words:\n",
    "            if word not in stopwords or word in sentiment_words:\n",
    "                filtered_words.append(word)  \n",
    "        return ' '.join(filtered_words) \n",
    "    return text\n",
    "\n",
    "sentiment_words = [\n",
    "    \"نہیں\", \"برا\", \"خراب\", \"نفرت\", \"اچھا\", \"کامیاب\", \"خوش\", \n",
    "    \"پیار\", \"محبت\", \"تعریف\", \"عظیم\", \"خوبصورت\", \"مایوس\"\n",
    "    # Add more based on your needs\n",
    "]\n",
    "df['Cleaned Text'] = df['Text'].apply(lambda x: remove_stopwords(x, combined_stopwords_list, sentiment_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a3aa2",
   "metadata": {},
   "source": [
    "# 2) Punctuation, Emojis, and Hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "19a31918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # New sentiment dictionary\n",
    "emoji_sentiment_dict = {\n",
    "    \"positive\": [\n",
    "        \"😊\", \"😁\", \"😂\", \"🤣\", \"😃\", \"😄\", \"😅\", \"😆\", \"😉\", \"😍\",\n",
    "        \"😘\", \"🥰\", \"😻\", \"😼\", \"😽\", \"😇\", \"🤗\", \"🤩\", \"🙌\", \"👍\",\n",
    "        \"👏\", \"👐\", \"✌️\", \"🤝\", \"🤞\", \"🤟\", \"🤘\", \"✋\", \"👌\", \n",
    "        \"✨\", \"🎉\", \"🎊\", \"🌈\", \"🌟\", \"🌻\", \"🌼\", \"🍀\", \"🍎\", \"🍉\", \n",
    "        \"🥳\", \"🥇\", \"💖\", \"💞\", \"💗\", \"💫\", \"🦄\", \"🐶\", \"🐱\", \"🐰\", \n",
    "        \"🐼\", \"🐸\", \"🍩\", \"🍕\", \"🍣\", \"🍦\", \"☕\", \"🌍\", \"🌞\", \"🔥\", \n",
    "        \"🥂\", \"🎈\", \"🌸\", \"🌷\", \"🌹\", \"🌺\", \"💐\", \"💖\", \"💗\", \"😹\",\n",
    "        \"❤️\", \"💘\", \"💞\", \"💓\", \"💝\", \"💟\", \"🧡\", \"😎\",\"😜\"\n",
    "        \"🤍\", \"💜\", \"💚\", \"💛\", \"💙\", \"🩹\", \"😝\", \"🙏\", \"🤲\",\"💯\"\n",
    "    ],\n",
    "    \"negative\": [\n",
    "        \"😞\", \"😠\", \"😡\", \"😢\", \"😭\", \"😩\", \"😤\", \"😳\", \"😔\", \"😒\", \n",
    "        \"😣\", \"😵\", \"😱\", \"🥴\", \"😈\", \"👿\", \"👎\", \"😖\", \"😫\", \n",
    "        \"😬\", \"💩\", \"🌧️\", \"🌩️\", \"🌪️\", \"👺\", \"⚰️\", \"☠️\", \"😾\", \n",
    "        \"😿\", \"👻\", \"😵‍💫\", \"😧\",\"💔\"\n",
    "    ],\n",
    "    \"neutral\": [\n",
    "        \"😐\", \"😑\", \"😶\", \"😏\", \"🤨\", \"😕\", \"🙄\", \"😬\", \"😮\", \"😯\", \n",
    "        \"😴\", \"😪\", \"🤔\", \"🤷‍♂️\", \"🤷‍♀️\", \"🧐\", \"😸\", \"😺\", \"🦁\", \n",
    "        \"🌍\", \"🌎\", \"🌏\", \"🍔\", \"🍟\", \"🌮\", \"🌯\", \"🥗\", \"🌭\", \n",
    "        \"🍜\", \"🧋\", \"🚗\", \"🚕\", \"🚙\", \"🚌\", \"🚎\", \"🚲\", \"🏠\", \n",
    "        \"✋\", \"🖐️\", \"🖖\", \"🤚\", \"👋\", \"🤙\", \"👈\", \"👉\", \"👆\", \"👇\", \n",
    "        \"✊\", \"✌️\", \"🤞\", \"🤟\", \"🖐️\", \"👌\", \"👐\", \"🙌\", \"👊\", \"📝\",\n",
    "        \"🤜\", \"🤛\", \"🦾\", \"✍️\", \"💪\", \"🦵\", \"🦶\", \"⚖️\", \"🔍\", \n",
    "        \"🔒\", \"🔑\", \"🔓\", \"💡\", \"🔋\", \"🔌\", \"📩\", \"✉️\", \"🔗\", \"😓\",\"😲\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def clean_text_with_sentiment(text):\n",
    "    sentiments = []  \n",
    "    if pd.isna(text):\n",
    "        return ''  \n",
    "   \n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE) #remove hyperlinks\n",
    "    text = re.sub(r\"\\d+\", '', text)  # Remove numerics\n",
    "    text = re.sub(r\"[^\\w\\s\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF]+\", '', text) #remove punctuation and symbols\n",
    "    text = re.sub(r\"@\\w+\", '', text)  # Remove mentions\n",
    "    text = re.sub(r\"#\\w+\", '', text)  # Remove hashtags   \n",
    "    text = re.sub(r'\\b[A-Za-zA-Z]+\\b', '', text)  #Remove English Words\n",
    "    text = re.sub(r'_', '', text) #Remove underscores\n",
    "    \n",
    "    for sentiment, emojis in emoji_sentiment_dict.items():\n",
    "        for emoji in emojis:\n",
    "            if emoji in text:\n",
    "                sentiments.append(sentiment)  # Append the sentiment (positive/negative/neutral) in sentiments list for each document\n",
    "                text = text.replace(emoji, '')  # Remove the emoji from the text\n",
    "            \n",
    "    text = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF]', '', text)  # Remove unmatched emojis\n",
    "\n",
    "    text = ' '.join(text.split()) #remove the spaces by splitting and then join the words again with 1 space\n",
    "    sentiment_str = ' '.join(sentiments) if sentiments else 'NoSentiment' #if the list contains some sentiments,join with single space as a string \n",
    "    \n",
    "    return f\"{text.strip()} {sentiment_str}\"  # Combine cleaned text with sentiment\n",
    "\n",
    "df['Cleaned Text'] = df['Cleaned Text'].apply(clean_text_with_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917d882",
   "metadata": {},
   "source": [
    "# 3) Short Conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a7803c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment distribution after assignment:\n",
      "sentiment\n",
      "positive    47521\n",
      "neutral     23280\n",
      "negative     7204\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def filter_short_posts(text):\n",
    "    if len(text.split()) < 3:\n",
    "        return None\n",
    "    return text\n",
    "\n",
    "# Apply the filter function to clean the text\n",
    "df['Cleaned Text'] = df['Cleaned Text'].apply(filter_short_posts)\n",
    "df.dropna(subset=['Cleaned Text'], inplace=True)\n",
    "\n",
    "# Create the sentiment column\n",
    "def assign_sentiment(text):\n",
    "    if 'NoSentiment' in text:\n",
    "        return 'neutral'\n",
    "    elif 'positive' in text:\n",
    "        return 'positive'\n",
    "    elif 'negative' in text:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'  # Default case if none match\n",
    "\n",
    "df['sentiment'] = df['Cleaned Text'].apply(assign_sentiment) #create new column\n",
    "\n",
    "# Remove sentiment words from 'Cleaned Text'\n",
    "df['Cleaned Text'] = df['Cleaned Text'].replace({'positive': '', 'negative': '', 'neutral': '', 'NoSentiment': ''}, regex=True)\n",
    "\n",
    "# Check sentiment distribution\n",
    "print(\"Sentiment distribution after assignment:\")\n",
    "\n",
    "# Save to CSV\n",
    "df[['Cleaned Text', 'sentiment']].to_csv('C:/Users/admin/OneDrive/Desktop/SEMESTER7/NLP_ass#1/Cleaned Tweets Dataset.csv', index=False, encoding='utf-8-sig')\n",
    "print(df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b5f699",
   "metadata": {},
   "source": [
    "# 1) Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3136e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "urdu_stem_dict = {\n",
    "    # Verbs\n",
    "    \"کرتے\": \"کر\", \"کرتی\": \"کر\", \"کرتا\": \"کر\", \"کرنا\": \"کر\", \"کررہا\": \"کر\", \"کررہی\": \"کر\", \"کررہے\": \"کر\",\n",
    "    \"گیا\": \"جا\", \"گئی\": \"جا\", \"گئے\": \"جا\", \"جانا\": \"جا\", \"جاتا\": \"جا\", \"جاتی\": \"جا\", \"جاتے\": \"جا\", \n",
    "    \"پڑھتا\": \"پڑھ\", \"پڑھتی\": \"پڑھ\", \"پڑھتے\": \"پڑھ\", \"پڑھنا\": \"پڑھ\", \"پڑھا\": \"پڑھ\", \"پڑھی\": \"پڑھ\",\n",
    "    \"دیتا\": \"دے\", \"دیتی\": \"دے\", \"دیتے\": \"دے\", \"دینا\": \"دے\", \"دے\": \"دے\",\n",
    "    \"لکھتا\": \"لکھ\", \"لکھتی\": \"لکھ\", \"لکھتے\": \"لکھ\", \"لکھنا\": \"لکھ\", \"لکھا\": \"لکھ\", \"لکھی\": \"لکھ\",\n",
    "    \"دیکھتا\": \"دیکھ\", \"دیکھتی\": \"دیکھ\", \"دیکھتے\": \"دیکھ\", \"دیکھنا\": \"دیکھ\", \"دیکھا\": \"دیکھ\", \"دیکھی\": \"دیکھ\",\n",
    "    \"چلتا\": \"چل\", \"چلتی\": \"چل\", \"چلتے\": \"چل\", \"چلنا\": \"چل\", \"چلا\": \"چل\", \"چلی\": \"چل\",\n",
    "    \"بولتا\": \"بول\", \"بولتی\": \"بول\", \"بولتے\": \"بول\", \"بولنا\": \"بول\", \"بولا\": \"بول\", \"بولی\": \"بول\",\n",
    "    \"سنتا\": \"سن\", \"سنتی\": \"سن\", \"سنتے\": \"سن\", \"سننا\": \"سن\", \"سنا\": \"سن\", \"سنی\": \"سن\",\n",
    "    \"پہنچتا\": \"پہنچ\", \"پہنچتی\": \"پہنچ\", \"پہنچتے\": \"پہنچ\", \"پہنچنا\": \"پہنچ\", \"پہنچا\": \"پہنچ\", \"پہنچی\": \"پہنچ\",\n",
    "    \"خریدتا\": \"خرید\", \"خریدتی\": \"خرید\", \"خریدتے\": \"خرید\", \"خریدنا\": \"خرید\", \"خریدا\": \"خرید\", \"خریدی\": \"خرید\",\n",
    "    \"بیچتا\": \"بیچ\", \"بیچتی\": \"بیچ\", \"بیچتے\": \"بیچ\", \"بیچنا\": \"بیچ\", \"بیچا\": \"بیچ\", \"بیچی\": \"بیچ\",\n",
    "    \"آتا\": \"آ\", \"آتی\": \"آ\", \"آتے\": \"آ\", \"آنا\": \"آ\", \"آیا\": \"آ\", \"آئی\": \"آ\",\n",
    "    \"ہوتا\": \"ہو\", \"ہوتی\": \"ہو\", \"ہوتے\": \"ہو\", \"ہونا\": \"ہو\", \"ہوا\": \"ہو\", \"ہوئی\": \"ہو\",\n",
    "    \"جاتا\": \"جا\", \"جاتی\": \"جا\", \"جاتے\": \"جا\", \"چاہتا\": \"چاہ\", \"چاہتی\": \"چاہ\", \"چاہتے\": \"چاہ\",\n",
    "    \"کہتا\": \"کہ\", \"کہتی\": \"کہ\", \"کہتے\": \"کہ\", \"کہنا\": \"کہ\", \"کہا\": \"کہ\",\"کہتیں\": \"کہ\",\n",
    "    \"سناتا\": \"سنا\", \"سناتی\": \"سنا\", \"سناتے\": \"سنا\", \"سننا\": \"سن\", \"سنایا\": \"سنا\",\n",
    "    \"پکڑتا\": \"پکڑ\", \"پکڑتی\": \"پکڑ\", \"پکڑتے\": \"پکڑ\", \"پکڑنا\": \"پکڑ\", \"پکڑا\": \"پکڑ\", \"پکڑی\": \"پکڑ\",\n",
    "    \n",
    "    # Adjectives\n",
    "    \"اچھا\": \"اچھا\", \"اچھی\": \"اچھا\", \"اچھے\": \"اچھا\", \"خوبصورت\": \"خوبصورت\", \"بڑا\": \"بڑا\", \"بڑی\": \"بڑا\", \"بڑے\": \"بڑا\",\n",
    "    \"چھوٹا\": \"چھوٹا\", \"چھوٹی\": \"چھوٹا\", \"چھوٹے\": \"چھوٹا\", \"لمبا\": \"لمبا\", \"لمبی\": \"لمبا\", \"لمبے\": \"لمبا\",\n",
    "    \"پیارا\": \"پیارا\", \"پیارے\": \"پیارا\", \"پیاری\": \"پیارا\", \"عظیم\": \"عظیم\", \"بری\": \"برا\", \"برے\": \"برا\", \"برا\": \"برا\",\n",
    "    \n",
    "    # Nouns\n",
    "    \"لڑکا\": \"لڑکا\", \"لڑکی\": \"لڑکی\", \"لڑکے\": \"لڑکا\", \"لڑکوں\": \"لڑکا\", \"لڑکیوں\": \"لڑکی\", \n",
    "    \"گھر\": \"گھر\", \"کتاب\": \"کتاب\", \"کتابیں\": \"کتاب\", \"استاد\": \"استاد\", \"استادوں\": \"استاد\", \"شاگرد\": \"شاگرد\",\n",
    "    \"دوست\": \"دوست\", \"دوستوں\": \"دوست\", \"پہلی\": \"پہلا\", \"پہلے\": \"پہلا\", \"شہر\": \"شہر\", \"شہروں\": \"شہر\", \n",
    "    \"آدمی\": \"آدمی\", \"آدمیوں\": \"آدمی\", \"دوکان\": \"دوکان\", \"دوکانیں\": \"دوکان\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2986fe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                  جزاک خیر خوشی مقدر دعا قبول فرمائے آمین\n",
      "1                                                    سچ اے\n",
      "2                                        بیکری بند بند گھر\n",
      "3                                              غلط نہیں کہ\n",
      "4                                              جاہل پٹواری\n",
      "                               ...                        \n",
      "83609       بات کا مزہ تب آ آدھی اردو آدھی پنجابی مکس بولے\n",
      "83610    مریم بھگوڑی کا نہتی لڑکی کیڑا مر جا عزت خاتون ...\n",
      "83612    فوجہےتوپاکستانہے بلاول زرداری اوقات میرے پاک ف...\n",
      "83613         انسان سستا محبت نام بکتا مہنگی انسان محبت ہے\n",
      "83614                               اداسی چیختی قہقہوں درد\n",
      "Name: stemmed_text, Length: 78005, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def stem_text(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    stemmed_words = []  # Initialize a list for stemmed words\n",
    "    for word in words:\n",
    "        # Get the stemmed word or keep the original if not found\n",
    "        stemmed_words.append(urdu_stem_dict.get(word, word))\n",
    "    return ' '.join(stemmed_words)  # Join and return the stemmed text\n",
    "\n",
    "df['stemmed_text'] = df['Cleaned Text'].apply(stem_text)\n",
    "df[['Cleaned Text', 'sentiment','stemmed_text']].to_csv('C:/Users/admin/OneDrive/Desktop/SEMESTER7/NLP_ass#1/Cleaned Tweets Dataset.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(df[ 'stemmed_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e7e85",
   "metadata": {},
   "source": [
    "# 2) Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40cef5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "urdu_lemma_dict = {\n",
    "    # Verbs\n",
    "    \"کرتے\": \"کرنا\", \"کرتی\": \"کرنا\", \"کرتا\": \"کرنا\", \"کررہا\": \"کرنا\", \"کررہی\": \"کرنا\", \"کررہے\": \"کرنا\",\n",
    "    \"گیا\": \"جانا\", \"گئی\": \"جانا\", \"گئے\": \"جانا\", \"جاتا\": \"جانا\", \"جاتی\": \"جانا\", \"جاتے\": \"جانا\",\n",
    "    \"پڑھتا\": \"پڑھنا\", \"پڑھتی\": \"پڑھنا\", \"پڑھتے\": \"پڑھنا\", \"پڑھا\": \"پڑھنا\", \"پڑھی\": \"پڑھنا\", \"پڑھنا\": \"پڑھنا\",\n",
    "    \"دیتا\": \"دینا\", \"دیتی\": \"دینا\", \"دیتے\": \"دینا\", \"دینا\": \"دینا\", \"لکھتا\": \"لکھنا\", \"لکھتی\": \"لکھنا\", \n",
    "    \"لکھتے\": \"لکھنا\", \"لکھا\": \"لکھنا\", \"لکھی\": \"لکھنا\", \"لکھنا\": \"لکھنا\",\n",
    "    \"دیکھتا\": \"دیکھنا\", \"دیکھتی\": \"دیکھنا\", \"دیکھتے\": \"دیکھنا\", \"دیکھا\": \"دیکھنا\", \"دیکھی\": \"دیکھنا\",\n",
    "    \"چلتا\": \"چلنا\", \"چلتی\": \"چلنا\", \"چلتے\": \"چلنا\", \"چلنا\": \"چلنا\", \"بولتا\": \"بولنا\", \"بولتی\": \"بولنا\", \n",
    "    \"بولتے\": \"بولنا\", \"بولنا\": \"بولنا\", \"سنتا\": \"سننا\", \"سنتی\": \"سننا\", \"سنتے\": \"سننا\", \"سنا\": \"سننا\",\n",
    "    \"پہنچتا\": \"پہنچنا\", \"پہنچتی\": \"پہنچنا\", \"پہنچتے\": \"پہنچنا\", \"پہنچا\": \"پہنچنا\", \"پہنچی\": \"پہنچنا\",\n",
    "    \"خریدتا\": \"خریدنا\", \"خریدتی\": \"خریدنا\", \"خریدتے\": \"خریدنا\", \"خریدا\": \"خریدنا\", \"خریدی\": \"خریدنا\",\n",
    "    \"بیچتا\": \"بیچنا\", \"بیچتی\": \"بیچنا\", \"بیچتے\": \"بیچنا\", \"بیچا\": \"بیچنا\", \"بیچی\": \"بیچنا\",\n",
    "    \"آتا\": \"آنا\", \"آتی\": \"آنا\", \"آتے\": \"آنا\", \"آیا\": \"آنا\", \"آئی\": \"آنا\",\n",
    "    \"ہوتا\": \"ہونا\", \"ہوتی\": \"ہونا\", \"ہوتے\": \"ہونا\", \"ہوا\": \"ہونا\", \"ہوئی\": \"ہونا\",\n",
    "    \"جاتا\": \"جانا\", \"جاتی\": \"جانا\", \"جاتے\": \"جانا\", \"چاہتا\": \"چاہنا\", \"چاہتی\": \"چاہنا\", \"چاہتے\": \"چاہنا\",\n",
    "    \"کہتا\": \"کہنا\", \"کہتی\": \"کہنا\", \"کہتے\": \"کہنا\", \"کہا\": \"کہنا\", \"کہنا\": \"کہنا\",\n",
    "    \"پکڑتا\": \"پکڑنا\", \"پکڑتی\": \"پکڑنا\", \"پکڑتے\": \"پکڑنا\", \"پکڑا\": \"پکڑنا\", \"پکڑی\": \"پکڑنا\",\n",
    "    \"سناتا\": \"سننا\", \"سناتی\": \"سننا\", \"سناتے\": \"سننا\", \"سنایا\": \"سننا\", \"سننا\": \"سننا\",\n",
    "    \n",
    "    # Adjectives\n",
    "    \"اچھا\": \"اچھا\", \"اچھی\": \"اچھا\", \"اچھے\": \"اچھا\", \"خوبصورت\": \"خوبصورت\", \"بڑا\": \"بڑا\", \"بڑی\": \"بڑا\", \"بڑے\": \"بڑا\",\n",
    "    \"چھوٹا\": \"چھوٹا\", \"چھوٹی\": \"چھوٹا\", \"چھوٹے\": \"چھوٹا\", \"لمبا\": \"لمبا\", \"لمبی\": \"لمبا\", \"لمبے\": \"لمبا\",\n",
    "    \"پیارا\": \"پیارا\", \"پیارے\": \"پیارا\", \"پیاری\": \"پیارا\", \"عظیم\": \"عظیم\", \"برا\": \"برا\", \"بری\": \"برا\", \"برے\": \"برا\",\n",
    "    \"چھوٹا\": \"چھوٹا\", \"لمبا\": \"لمبا\", \"کمزور\": \"کمزور\", \"طاقتور\": \"طاقتور\", \"ذہین\": \"ذہین\", \"سست\": \"سست\", \"محنتی\": \"محنتی\",\n",
    "    \n",
    "    # Nouns\n",
    "    \"لڑکا\": \"لڑکا\", \"لڑکی\": \"لڑکی\", \"لڑکے\": \"لڑکا\", \"لڑکوں\": \"لڑکا\", \"لڑکیوں\": \"لڑکی\", \n",
    "    \"گھر\": \"گھر\", \"کتاب\": \"کتاب\", \"کتابیں\": \"کتاب\", \"استاد\": \"استاد\", \"استادوں\": \"استاد\", \n",
    "    \"شاگرد\": \"شاگرد\", \"دوست\": \"دوست\", \"دوستوں\": \"دوست\", \"پہلی\": \"پہلا\", \"پہلے\": \"پہلا\", \n",
    "    \"شہر\": \"شہر\", \"شہروں\": \"شہر\", \"آدمی\": \"آدمی\", \"آدمیوں\": \"آدمی\", \"دوکان\": \"دوکان\", \"دوکانیں\": \"دوکان\",\n",
    "    \"لڑکیاں\": \"لڑکی\", \"دوستیاں\": \"دوستی\", \"اساتذہ\": \"استاد\", \"مددگار\": \"مددگار\", \"بچہ\": \"بچہ\", \n",
    "    \"بچے\": \"بچہ\", \"بچی\": \"بچہ\", \"بچیوں\": \"بچی\", \"کتابیں\": \"کتاب\", \"کپڑا\": \"کپڑا\", \"کپڑے\": \"کپڑا\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6fd3ae16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                  جزاک خیر خوشی مقدر دعا قبول فرمائے آمین\n",
      "1                                                    سچ اے\n",
      "2                                        بیکری بند بند گھر\n",
      "3                                           غلط نہیں کہتیں\n",
      "4                                              جاہل پٹواری\n",
      "                               ...                        \n",
      "83609     بات کا مزہ تب آنا آدھی اردو آدھی پنجابی مکس بولے\n",
      "83610    مریم بھگوڑی کا نہتی لڑکی کیڑا مر جانا عزت خاتو...\n",
      "83612    فوجہےتوپاکستانہے بلاول زرداری اوقات میرے پاک ف...\n",
      "83613         انسان سستا محبت نام بکتا مہنگی انسان محبت ہے\n",
      "83614                               اداسی چیختی قہقہوں درد\n",
      "Name: lemmatized_text, Length: 78005, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    lemmatized_words = []  # Initialize a list for lemmatized words\n",
    "    for word in words:\n",
    "        # Get the lemmatized word or keep the original if not found\n",
    "        lemmatized_words.append(urdu_lemma_dict.get(word, word))\n",
    "    return ' '.join(lemmatized_words)  # Join and return the lemmatized text\n",
    "\n",
    "df['lemmatized_text'] = df['Cleaned Text'].apply(lemmatize_text)\n",
    "\n",
    "df[['Cleaned Text', 'sentiment','stemmed_text','lemmatized_text']].to_csv('C:/Users/admin/OneDrive/Desktop/SEMESTER7/NLP_ass#1/Cleaned Tweets Dataset.csv', index=False, encoding='utf-8-sig')\n",
    "print(df[ 'lemmatized_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d0148",
   "metadata": {},
   "source": [
    "# 1) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51e983c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [جزاک, خیر, خوشی, مقدر, دعا, قبول, فرمائے, آمین]\n",
      "1                                              [سچ, اے]\n",
      "2                                [بیکری, بند, بند, گھر]\n",
      "3                                    [غلط, نہیں, کہتیں]\n",
      "4                                        [جاہل, پٹواری]\n",
      "5     [رات, سونے, ریٹویٹ, فالوورز, کا, اضافہ, یقینی,...\n",
      "6                    [مریم, نواز, طرح, اسکے, حسن, جیلس]\n",
      "7       [رخصت, آنکھ, ملا, نہیں, جانا, جانا, نہیں, جانا]\n",
      "9                                   [تانوں, صحیح, بلاک]\n",
      "10                         [وی, کھول, آدھے, گل, اچ, پا]\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize the cleaned_text column\n",
    "df['tokenized_text'] = df['lemmatized_text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Display the result\n",
    "print(df['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359acde4",
   "metadata": {},
   "source": [
    "# 2) Tf-IDF (Term Frequency-Inverse Document Frequency):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "345516cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TF-IDF Score\n",
      "نہیں    2045.992794\n",
      "کا      1669.614345\n",
      "ہے      1149.234102\n",
      "نہ       897.683708\n",
      "جی       767.512248\n",
      "جانا     757.707836\n",
      "گا       720.920963\n",
      "شکریہ    710.499625\n",
      "فالو     710.102811\n",
      "بات      703.444939\n"
     ]
    }
   ],
   "source": [
    "#words that are both frequent within specific documents and unique across the whole set\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(df['lemmatized_text'])  #fit the column and transform it into a TF-IDF sparse matrix\n",
    "\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0) #sum of each word's value across all documents so it can be represented by 1 value\n",
    "\n",
    "tfidf_scores_transposed = tfidf_scores.T #turn into a  columns so that it can be stored as dataframe column\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out() #get feature name to label each score with the word it represents\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_scores_transposed, index=feature_names, columns=[\"TF-IDF Score\"])\n",
    "\n",
    "top_10_df = tfidf_df.sort_values(by=\"TF-IDF Score\", ascending=False).head(10)\n",
    "\n",
    "print(top_10_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5409cf4e",
   "metadata": {},
   "source": [
    "# 3) Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2dd605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec with vector_size=50, window=3, min_count=1\n",
      "[('سوری', 0.8570399880409241), ('کتنا', 0.8381215929985046), ('آج', 0.8362111449241638), ('سوچو', 0.8347245454788208), ('دفع', 0.8245216608047485)]\n",
      "Training Word2Vec with vector_size=50, window=3, min_count=3\n",
      "[('آج', 0.8205878734588623), ('سوری', 0.8168452382087708), ('سمجھے', 0.7721920609474182), ('کتنا', 0.7716867327690125), ('سوچتا', 0.7678647637367249)]\n",
      "Training Word2Vec with vector_size=50, window=3, min_count=5\n",
      "[('سمجھدار', 0.7853494882583618), ('سوری', 0.7838637828826904), ('خاصہ', 0.7731002569198608), ('کتنا', 0.7720376253128052), ('ہاہا', 0.7590405344963074)]\n",
      "Training Word2Vec with vector_size=50, window=5, min_count=1\n",
      "[('سوری', 0.8486835360527039), ('سوچتا', 0.8366994261741638), ('کتنا', 0.8316372632980347), ('سمجھا', 0.822475790977478), ('خاصہ', 0.8171761631965637)]\n",
      "Training Word2Vec with vector_size=50, window=5, min_count=3\n",
      "[('سوری', 0.836127519607544), ('آج', 0.7995980978012085), ('کتنا', 0.7899698615074158), ('خاصہ', 0.7738021612167358), ('اسلیے', 0.7724568843841553)]\n",
      "Training Word2Vec with vector_size=50, window=5, min_count=5\n",
      "[('سوری', 0.8167716264724731), ('سمجھدار', 0.7825472950935364), ('بھئ', 0.7746327519416809), ('پہچانا', 0.7692732214927673), ('سوچتا', 0.7655954957008362)]\n",
      "Training Word2Vec with vector_size=50, window=10, min_count=1\n",
      "[('سوچتا', 0.8410241007804871), ('سوچو', 0.8336207866668701), ('خاصہ', 0.8171529769897461), ('پہچانا', 0.8161892890930176), ('سوری', 0.8093863725662231)]\n",
      "Training Word2Vec with vector_size=50, window=10, min_count=3\n",
      "[('سوری', 0.8039236068725586), ('خاصہ', 0.7923720479011536), ('ریلیکس', 0.7556853294372559), ('موڈ', 0.7556719183921814), ('خاصا', 0.7421157956123352)]\n",
      "Training Word2Vec with vector_size=50, window=10, min_count=5\n",
      "[('کنکر', 0.7878488898277283), ('سوچتا', 0.7859497666358948), ('خاصہ', 0.7807072401046753), ('سوری', 0.7697164416313171), ('پہچانا', 0.7599198818206787)]\n",
      "Training Word2Vec with vector_size=100, window=3, min_count=1\n",
      "[('آج', 0.8381640315055847), ('سوری', 0.8342348337173462), ('کتنا', 0.8182226419448853), ('غلط', 0.7893035411834717), ('سوچو', 0.7892692685127258)]\n",
      "Training Word2Vec with vector_size=100, window=3, min_count=3\n",
      "[('سوری', 0.7958413362503052), ('خاصہ', 0.7493095993995667), ('کتنا', 0.7466647028923035), ('آج', 0.7302533984184265), ('سمجھا', 0.718113899230957)]\n",
      "Training Word2Vec with vector_size=100, window=3, min_count=5\n",
      "[('سوری', 0.7504395246505737), ('خاصہ', 0.7277730703353882), ('عروج', 0.7042909264564514), ('ہاہا', 0.7029862403869629), ('آج', 0.7015751600265503)]\n",
      "Training Word2Vec with vector_size=100, window=5, min_count=1\n",
      "[('سوری', 0.81003737449646), ('خاصہ', 0.777645468711853), ('سمجھدار', 0.7721352577209473), ('ہاہا', 0.7693688273429871), ('کتنا', 0.7683664560317993)]\n",
      "Training Word2Vec with vector_size=100, window=5, min_count=3\n",
      "[('سوری', 0.7465091943740845), ('خاصہ', 0.7077512145042419), ('سوچتا', 0.6920329332351685), ('سوچو', 0.6781362295150757), ('چھوٹا', 0.6738144159317017)]\n",
      "Training Word2Vec with vector_size=100, window=5, min_count=5\n",
      "[('سوری', 0.6962188482284546), ('خاصہ', 0.6762896180152893), ('سوچتا', 0.6460142135620117), ('سوچو', 0.6415049433708191), ('خاصا', 0.6317492723464966)]\n",
      "Training Word2Vec with vector_size=100, window=10, min_count=1\n",
      "[('خاصہ', 0.7387720942497253), ('سوری', 0.735414981842041), ('سوچو', 0.7070971131324768), ('کنکر', 0.7065240740776062), ('جوتا', 0.7005444169044495)]\n",
      "Training Word2Vec with vector_size=100, window=10, min_count=3\n",
      "[('خاصہ', 0.6906906962394714), ('سوری', 0.6879600882530212), ('کنکر', 0.6516314744949341), ('سوچو', 0.6420167684555054), ('سوچتا', 0.6360601782798767)]\n",
      "Training Word2Vec with vector_size=100, window=10, min_count=5\n",
      "[('خاصہ', 0.6513169407844543), ('کنکر', 0.6377193927764893), ('سوچو', 0.6350883841514587), ('سوری', 0.6252183318138123), ('خاصا', 0.6181495189666748)]\n",
      "Training Word2Vec with vector_size=200, window=3, min_count=1\n",
      "[('سوری', 0.8339028358459473), ('آج', 0.8033279776573181), ('کتنا', 0.8032545447349548), ('سمجھدار', 0.7989997267723083), ('خاصہ', 0.798215389251709)]\n",
      "Training Word2Vec with vector_size=200, window=3, min_count=3\n",
      "[('سوری', 0.748590350151062), ('آج', 0.7379738092422485), ('غلط', 0.7296521067619324), ('خاصہ', 0.7234534621238708), ('سوچو', 0.7183849811553955)]\n",
      "Training Word2Vec with vector_size=200, window=3, min_count=5\n",
      "[('سوری', 0.7430304288864136), ('آج', 0.7099363207817078), ('خاصہ', 0.7018942832946777), ('سوچو', 0.6851989030838013), ('سوچتا', 0.6772058606147766)]\n",
      "Training Word2Vec with vector_size=200, window=5, min_count=1\n",
      "[('سوری', 0.7690261602401733), ('سوچو', 0.7552818655967712), ('خاصہ', 0.7521668076515198), ('سوچتا', 0.7503336071968079), ('اپکا', 0.7447690963745117)]\n",
      "Training Word2Vec with vector_size=200, window=5, min_count=3\n",
      "[('سوری', 0.7540677785873413), ('خاصہ', 0.7000635862350464), ('سوچتا', 0.6861211657524109), ('سوچو', 0.6754598617553711), ('کنکر', 0.672144889831543)]\n",
      "Training Word2Vec with vector_size=200, window=5, min_count=5\n",
      "[('سوری', 0.7218796014785767), ('خاصہ', 0.6937547922134399), ('کنکر', 0.6536691784858704), ('خاصا', 0.648831307888031), ('سوچو', 0.6476299166679382)]\n",
      "Training Word2Vec with vector_size=200, window=10, min_count=1\n",
      "[('خاصہ', 0.7210817337036133), ('سوری', 0.708031415939331), ('سوچو', 0.6854141354560852), ('سوچتا', 0.6769299507141113), ('اسلیے', 0.6762131452560425)]\n",
      "Training Word2Vec with vector_size=200, window=10, min_count=3\n",
      "[('سوری', 0.6461931467056274), ('خاصہ', 0.6362409591674805), ('سوچو', 0.6276546120643616), ('اپکا', 0.6123459339141846), ('سوچتا', 0.6031367182731628)]\n",
      "Training Word2Vec with vector_size=200, window=10, min_count=5\n",
      "[('خاصہ', 0.6278622150421143), ('خاصا', 0.6116315722465515), ('کنکر', 0.5993087291717529), ('سوری', 0.598421573638916), ('سوچو', 0.5854251384735107)]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Sample hyperparameters to test\n",
    "vector_sizes = [50, 100, 200]\n",
    "windows = [3, 5, 10]\n",
    "min_counts = [1, 3, 5]\n",
    "\n",
    "# Iterate over all combinations of parameters\n",
    "for vector_size, window, min_count in itertools.product(vector_sizes, windows, min_counts):\n",
    "    print(f\"Training Word2Vec with vector_size={vector_size}, window={window}, min_count={min_count}\")\n",
    "    model = Word2Vec(df['tokenized_text'], vector_size=vector_size, window=window, min_count=min_count, workers=4, sg=1)# sg=0 means CBOW; 1 means Skip-gram\n",
    "    similar_words = model.wv.most_similar('اچھا', topn=5)\n",
    "    print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6485ddb",
   "metadata": {},
   "source": [
    "# PHASE-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18522a5b",
   "metadata": {},
   "source": [
    "# 1) Unigram, Bigram, and Trigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9fdb5c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common unigrams:\n",
      "نہیں: 14983\n",
      "کا: 14666\n",
      "ہے: 7038\n",
      "نہ: 5316\n",
      "جانا: 4009\n",
      "گا: 3900\n",
      "بات: 3299\n",
      "جی: 3256\n",
      "فالو: 3172\n",
      "دل: 3112\n",
      "\n",
      "Top 10 most common bigrams:\n",
      "('فالو', 'بیک'): 1397\n",
      "('صلی', 'علیہ'): 799\n",
      "('عمران', 'خان'): 781\n",
      "('فالو', 'فالو'): 766\n",
      "('نواز', 'شریف'): 743\n",
      "('سندھ', 'پولیس'): 729\n",
      "('والوں', 'فالو'): 670\n",
      "('علیہ', 'وآلہ'): 650\n",
      "('وآلہ', 'وسلم'): 605\n",
      "('کا', 'اضافہ'): 587\n",
      "\n",
      "Top 10 most common trigrams:\n",
      "('فالو', 'فالو', 'بیک'): 710\n",
      "('علیہ', 'وآلہ', 'وسلم'): 592\n",
      "('فالورز', 'کا', 'اضافہ'): 548\n",
      "('والوں', 'فالو', 'فالو'): 451\n",
      "('لسٹ', 'شامل', 'ہونے'): 438\n",
      "('ریٹوئیٹ', 'ریٹوئیٹ', 'والوں'): 438\n",
      "('ریٹوئیٹ', 'والوں', 'فالو'): 434\n",
      "('شامل', 'ہونے', 'لیے'): 425\n",
      "('ڈی', 'مینشن', 'ریٹوئیٹ'): 422\n",
      "('ہونے', 'لیے', 'ڈی'): 420\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists for unigrams, bigrams, and trigrams\n",
    "bigrams_list = []\n",
    "trigrams_list = []\n",
    "unigrams_list = []\n",
    "\n",
    "# Extract unigrams, bigrams, and trigrams\n",
    "for row in df['tokenized_text']:\n",
    "    # Collect unigrams\n",
    "    unigrams_list.extend(row)  # Append all items in the row to unigrams_list\n",
    "    \n",
    "    # Create bigrams\n",
    "    bigrams = list(ngrams(row, 2))\n",
    "    bigrams_list.extend(bigrams)  # Append all bigrams to the bigrams_list\n",
    "    \n",
    "    # Create trigrams\n",
    "    trigrams = list(ngrams(row, 3))\n",
    "    trigrams_list.extend(trigrams)  # Append all trigrams to the trigrams_list\n",
    "\n",
    "# Calculate frequency of unigrams, bigrams, and trigrams as key value pairs\n",
    "unigram_freq = Counter(unigrams_list)\n",
    "bigram_freq = Counter(bigrams_list)\n",
    "trigram_freq = Counter(trigrams_list)\n",
    "\n",
    "# Print the top 10 most common unigrams\n",
    "print(\"Top 10 most common unigrams:\")\n",
    "for unigram, freq in unigram_freq.most_common(10):\n",
    "    print(f\"{unigram}: {freq}\")\n",
    "\n",
    "# Print the top 10 most common bigrams\n",
    "print(\"\\nTop 10 most common bigrams:\")\n",
    "for bigram, freq in bigram_freq.most_common(10):\n",
    "    print(f\"{bigram}: {freq}\")\n",
    "\n",
    "# Print the top 10 most common trigrams\n",
    "print(\"\\nTop 10 most common trigrams:\")\n",
    "for trigram, freq in trigram_freq.most_common(10):\n",
    "    print(f\"{trigram}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a8ab25",
   "metadata": {},
   "source": [
    "# 1) Sentiment Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b07bb36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Training data\n",
      "Training Accuracy: 0.6752291519774374\n",
      "Training Precision: 0.6876243927794122\n",
      "Training Recall: 0.4498606178965136\n",
      "Training F1-score: 0.4595913163274701\n",
      "For Testing data\n",
      "Test Accuracy: 0.647073905518877\n",
      "Test Precision: 0.6125933726530964\n",
      "Test Recall: 0.41625204397328774\n",
      "Test F1-score: 0.41523061318236937\n"
     ]
    }
   ],
   "source": [
    "# Encode the sentiment labels\n",
    "le = LabelEncoder()\n",
    "df['sentiment'] = le.fit_transform(df['sentiment'])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tokenized_text'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=9000, ngram_range=(1, 2))\n",
    "X_train_vectors = vectorizer.fit_transform([' '.join(text) for text in X_train])\n",
    "X_test_vectors = vectorizer.transform([' '.join(text) for text in X_test])\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_vectors, y_train)\n",
    "\n",
    "y_train_pred = nb_model.predict(X_train_vectors)\n",
    "y_test_pred = nb_model.predict(X_test_vectors)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)#correctly predicts\n",
    "train_precision = precision_score(y_train, y_train_pred, average='macro')#how many of the predicted positives were actually positive.\n",
    "train_recall = recall_score(y_train, y_train_pred, average='macro') #how many actual positives your model caught.\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "print(\"For Training data\")\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Training Precision:\", train_precision)\n",
    "print(\"Training Recall:\", train_recall)\n",
    "print(\"Training F1-score:\", train_f1)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print(\"For Testing data\")\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "print(\"Test F1-score:\", test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b96160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d8362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a7a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d680b21d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
